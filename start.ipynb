{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x7f930d6434c0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "filepath = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "print(f\"Vocabulary size: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "            \n",
    "\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab=vocab)\n",
    "\n",
    "text = \"\"\"\n",
    "        \"It's the last he painted, you know, \"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Byte pair encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")   # TODO: implement own tokenizer\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})    # Vocabulary size GPT2: 50257\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 74, 1186, 824, 288, 1860, 67, 31811, 294, 3609, 11223]\n",
      "Ekretss ddddxxx thae je\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Ekretss ddddxxx thae je\"\n",
    "integers = tokenizer.encode(test_text)\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:    [290, 4920, 2241, 287]\n",
      "y:         [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:    {x}\")\n",
    "print(f\"y:         {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_lenght, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids)- max_lenght, stride):\n",
    "            input_chunk = token_ids[i:i + max_lenght]\n",
    "            target_chunk = token_ids[i + 1: i + max_lenght + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_lenght=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_lenght, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,    # Drop last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_lenght=4, stride=4, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "# second_batch = next(data_iter)\n",
    "# print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)    # Better than one-hot encoding - more efficient way of matrix multiplication in fully connecterd layer\n",
    "print(embedding_layer.weight)   # Initiated embedding layer with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Positional token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-independent embedding of tokens are better for reproducibility purposes.\n",
    "# However self-attention mechanism is also positio-agnostic (meaning it treats all tokens in a sequence equally regardless of their order)\n",
    "# it is helpful to injest positional information into the LLM\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Input shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_lenght = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_lenght=max_lenght, stride=max_lenght, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token ID: \\n\", inputs)\n",
    "print(\"\\nInput shape: \\n\", inputs.shape)    # first batch consists of 8 text examples, 4 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# 8 - examples\n",
    "# 4 - tokens for each example\n",
    "# 256 - embedding values for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# To add absolute embedding approach we add another layer of embeddings\n",
    "\n",
    "context_lenght = max_lenght\n",
    "pos_embedding_layer = torch.nn.Embedding(context_lenght, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_lenght))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# 4 - tokens in example\n",
    "# 256 - values of pos embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# we just add that two embedding layers\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# When adding positional embeddings to token embeddings, PyTorch automatically broadcasts the positional embeddings across the batch dimension. \n",
    "# Broadcasting expands the positional embeddings from 4×256 to 8×4×256 to match the shape of the token embeddings.\n",
    "# Duplicating positional embeddings for each batch example would be redundant and inefficient in terms of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Simple self attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your\n",
    "        [0.55, 0.87, 0.66], # journey\n",
    "        [0.57, 0.85, 0.64], # starts\n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.10], # one\n",
    "        [0.05, 0.80, 0.55], # step\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# 1. Multiplying input token with all tokens in a sequence -> it gives us floats\n",
    "query = inputs[1] # attention scores for word 'journey'\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)    # concise way of multiplying two vectors. It is a measure of similarity of one word to each words in a sequence\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# torch.dot(x_i, query) is essentially the same as:\n",
    "res = 0\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights normalized: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# 2. Normalize the results\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()    # we normalize but it is advisable to do that with softmax function\n",
    "print(f\"Attention weights normalized: {attn_weights_2_tmp}\")\n",
    "print(f\"Sum: {attn_weights_2_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores naive: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)   # softmax function assures that we don't get negative values. \n",
    "                                                    # this apprach however can have underflow or overflow. Therefore we use build in torch.softmax\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\"Attention scores naive: {attn_weights_2_naive}\")\n",
    "print(f\"Sum: {attn_weights_2_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores torch.softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\"Attention scores torch.softmax: {attn_weights_2}\")\n",
    "print(f\"Sum: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# 3. Now we can calculate context vector by multiplying each token vector values with corresponding attention weights \n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# To do that for all:\n",
    "# 1. Compute attention scores\n",
    "# 2. Compute attention weights\n",
    "# 3. Compute context vectors\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# More efficeint way is to use matrix multiplications:\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# We normalize\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1) # dim=-1 means we want to apply the softmax along the last dimension of attn_scores tensor\n",
    "print(attn_weights)\n",
    "\n",
    "# dim=-1: This is commonly used for normalizing attention scores so that each query focuses on different keys with probabilities summing to 1.\n",
    "# dim=-2: This is less common but could be used in scenarios where you want to normalize attention scores across queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All rows sums across columns: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = attn_weights[1].sum()\n",
    "print(f\"Row 2 sum: {row_2_sum}\")\n",
    "print(f\"All rows sums across columns: {attn_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we calculate context vector\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs[1] # same as context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Self attention mechanism with trainable weights - scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Introduction of Wq Wk Wv matrices\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]  # input embedding size 3\n",
    "d_out = 2               # output embedding size 2   # it is usually better to use same dims but for sake of learning the computations I leave 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "query_2= x_2 @ W_query\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_2 = query_2.dot(keys_2) # only for second token\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = query_2 @ keys.T    # all attention scores for second token\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectos_2 = attn_weights_2 @ values\n",
    "context_vectos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from self_attention import SelfAttention_V1, SelfAttention_V2, SelfAttention_V1_with_V2_weights\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_V1(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5085, 0.3508],\n",
       "        [0.5084, 0.3508],\n",
       "        [0.5084, 0.3506],\n",
       "        [0.5074, 0.3471],\n",
       "        [0.5076, 0.3446],\n",
       "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2 = SelfAttention_V2(d_in, d_out)\n",
    "torch.manual_seed(789)\n",
    "sa_v2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v12 = SelfAttention_V1_with_V2_weights(d_in, d_out)\n",
    "sa_v12(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Masking future words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1362, 0.1730, 0.1736, 0.1713, 0.1792, 0.1666],\n",
      "        [0.1359, 0.1730, 0.1735, 0.1716, 0.1790, 0.1670],\n",
      "        [0.1366, 0.1729, 0.1734, 0.1714, 0.1788, 0.1669],\n",
      "        [0.1493, 0.1701, 0.1704, 0.1697, 0.1732, 0.1674],\n",
      "        [0.1589, 0.1690, 0.1692, 0.1667, 0.1712, 0.1649],\n",
      "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_lenght = attn_scores.shape[0]   # 6\n",
    "mask_simple = torch.tril(torch.ones(context_lenght, context_lenght))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1362, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1359, 0.1730, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1366, 0.1729, 0.1734, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1493, 0.1701, 0.1704, 0.1697, 0.0000, 0.0000],\n",
       "        [0.1589, 0.1690, 0.1692, 0.1667, 0.1712, 0.0000],\n",
       "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n",
       "        [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n",
       "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_sum = masked_simple.sum(dim=-1, keepdim=True)  # i want to keep dimension like it is, not convert to list\n",
    "masked_simple_normalized = masked_simple / rows_sum # there is no informatin leakage because softmax with values 0 is not considering those values\n",
    "masked_simple_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],\n",
       "        [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],\n",
       "        [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],\n",
       "        [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is possible to do it more effectively by setting those values to -inf not to 0\n",
    "\n",
    "mask = torch.triu(torch.ones(context_lenght, context_lenght), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n",
       "        [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n",
       "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now what is left it to normalize it\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Dropout to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # elements are scaled by 1 / 0.5 = 2 to compensate\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1200, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7181, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4528, 0.5159, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3806, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3430, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Casual attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vec shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "from self_attention import CasualAttention\n",
    "torch.manual_seed(123)\n",
    "context_lenght = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_lenght, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"context_vec shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context_vecs: torch.Size([2, 6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from self_attention import MultiHeadAttentionWrapper\n",
    "torch.manual_seed(123)\n",
    "context_lenght = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, 6, 0.0, 2)\n",
    "context_vecs = mha(batch)\n",
    "print(f\"Shape of context_vecs: {context_vecs.shape}\")\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                [0.7179, 0.7058, 0.9156, 0.4340]\n",
    "            ],\n",
    "            [\n",
    "                [0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                [0.4606, 0.5159, 0.4220, 0.5786]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.3208, 1.1631, 1.2879],\n",
       "          [1.1631, 2.2150, 1.8424],\n",
       "          [1.2879, 1.8424, 2.0402]],\n",
       "\n",
       "         [[0.4391, 0.7003, 0.5903],\n",
       "          [0.7003, 1.3737, 1.0620],\n",
       "          [0.5903, 1.0620, 0.9912]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ a.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3208, 1.1631, 1.2879],\n",
       "        [1.1631, 2.2150, 1.8424],\n",
       "        [1.2879, 1.8424, 2.0402]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_head = a[0,0,:,:]\n",
    "first_res = first_head @ first_head.T\n",
    "first_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4391, 0.7003, 0.5903],\n",
       "        [0.7003, 1.3737, 1.0620],\n",
       "        [0.5903, 1.0620, 0.9912]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_head = a[0,1,:,:]\n",
    "second_res = second_head @ second_head.T\n",
    "second_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2940, 0.7409],\n",
       "         [0.3680, 0.3385],\n",
       "         [0.2490, 0.4221],\n",
       "         [0.2704, 0.2916],\n",
       "         [0.2888, 0.3418],\n",
       "         [0.2809, 0.3408]],\n",
       "\n",
       "        [[0.1934, 0.6825],\n",
       "         [0.1989, 0.6858],\n",
       "         [0.3221, 0.2964],\n",
       "         [0.2210, 0.4556],\n",
       "         [0.2384, 0.4428],\n",
       "         [0.2956, 0.3488]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from self_attention import MultiHeadAttention\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_lenght, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_lenght, 0.5, num_heads=2)\n",
    "context_vec = mha(batch)\n",
    "print(context_vec.shape)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import DummyGPTModel, GPT_CONFIG_124M\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "# model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "# logits = model(batch)\n",
    "# print(logits.shape)\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer normalization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import GPT_CONFIG_124M\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())    # rectified linear unit\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
       "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "STD: \n",
      "tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(f\"Mean: \\n{mean}\")\n",
    "print(f\"STD: \\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "STD: \n",
      "tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=False)  # do not use scientific notation\n",
    "print(f\"Mean: \\n{mean}\")\n",
    "print(f\"STD: \\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "tensor([[ 2.3842e-08],\n",
      "        [-9.5367e-08]], grad_fn=<MeanBackward1>)\n",
      "STD: \n",
      "tensor([[9.9978e-01],\n",
      "        [9.9991e-01]], grad_fn=<VarBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2294e-01, 7.0492e-01, -5.3015e-01, 1.5069e+00, -1.3587e+00],\n",
       "        [1.4247e+00, -8.1994e-01, -9.2480e-01, -6.7394e-01, 9.9397e-01]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import LayerNorm\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "torch.set_printoptions(sci_mode=True)  # do not use scientific notation\n",
    "print(f\"Mean: \\n{mean}\")\n",
    "print(f\"STD: \\n{var}\")\n",
    "out_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation function\n",
    "from models import GELU\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8910e-02,  1.2931e-02, -2.2646e-01,  ..., -1.5223e-02,\n",
       "          -1.8210e-03, -5.9271e-02],\n",
       "         [-2.9184e-02, -5.5144e-02, -2.9393e-01,  ..., -9.9391e-06,\n",
       "           4.3945e-02, -3.6865e-02],\n",
       "         [-1.2955e-01,  2.8791e-02, -3.8951e-01,  ..., -6.3852e-02,\n",
       "          -4.2675e-02,  1.5452e-02]],\n",
       "\n",
       "        [[ 4.8324e-03,  8.7444e-02, -2.4740e-01,  ...,  3.6467e-02,\n",
       "           6.4968e-02, -2.1661e-03],\n",
       "         [-1.4453e-02,  3.2953e-03, -2.8413e-01,  ...,  6.6434e-02,\n",
       "           1.1429e-02, -2.7560e-02],\n",
       "         [-2.2525e-02, -3.8481e-02, -2.3742e-01,  ..., -1.1999e-01,\n",
       "          -1.9021e-02,  5.4432e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import FeedForward\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient of 0.001531339017674327\n",
      "layers.1.0.weight has gradient of 0.0008734675939194858\n",
      "layers.2.0.weight has gradient of 0.0021116069983690977\n",
      "layers.3.0.weight has gradient of 0.0030934528913348913\n",
      "layers.4.0.weight has gradient of 0.007880646735429764\n"
     ]
    }
   ],
   "source": [
    "from models import ExampleDeepNeuralNetwork\n",
    "\n",
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([1. , 0., -1.])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([0.])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient of 0.24866612255573273\n",
      "layers.1.0.weight has gradient of 0.8006523847579956\n",
      "layers.2.0.weight has gradient of 0.3836198151111603\n",
      "layers.3.0.weight has gradient of 0.39542055130004883\n",
      "layers.4.0.weight has gradient of 1.0010857582092285\n"
     ]
    }
   ],
   "source": [
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "from models import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. GPT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output batch:\n",
      "tensor([[[ 0.0967, -0.0039, -0.2247,  ..., -0.0418, -0.1078,  0.1488],\n",
      "         [ 0.3412, -0.8254, -0.7412,  ..., -0.5463,  0.1970, -0.4303],\n",
      "         [ 0.6882, -0.1170, -0.2338,  ...,  0.1141, -0.6860,  0.0646],\n",
      "         [-0.3733,  0.2077, -0.1760,  ...,  1.1365,  0.4913, -0.5086]],\n",
      "\n",
      "        [[ 0.0709, -0.2920, -0.1572,  ..., -0.0820,  0.2679, -0.2921],\n",
      "         [ 0.0773, -0.2990, -0.3220,  ...,  1.1710,  0.0444,  0.1748],\n",
      "         [ 0.7489,  0.5121, -0.4035,  ...,  0.7211,  0.2885, -0.2390],\n",
      "         [-0.1525, -0.0339,  0.3752,  ...,  1.2968, -0.4658,  0.0257]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from models import GPTModel, GPT_CONFIG_124M, GPT_CONFIG_MEDIUM, GPT_CONFIG_LARGE, GPT_CONFIG_XL\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)  # do not use scientific notation\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(f\"Input batch:\\n{batch}\")\n",
    "print(f\"Output batch:\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nubmber of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total nubmber of parameters: {total_params:,}\") # not 124M parameters because of weight tying. Original transformer was reusing weight from embedding layer in output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token embedding layer shape: {model.tok_emb.weight.shape}\")\n",
    "print(f\"Output layer shape: {model.out_head.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_aprams_gpt2 = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())  # when we remove the count of paramters in output layer (50257*768=38597376) it is 124M parameters\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_aprams_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124412160"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params - 50257*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85026816"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.trf_blocks.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 1, parameters in attn: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "for i, block in enumerate(model.trf_blocks, start=1):\n",
    "    print(f\"Block: {i}, parameters in attn: {sum(p.numel() for p in block.attn.parameters()):,}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 1, parameters in ffn: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "for i, block in enumerate(model.trf_blocks, start=1):\n",
    "    print(f\"Block: {i}, parameters in ffn: {sum(p.numel() for p in block.ff.parameters()):,}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 # assuming float32, 4 bytes per parameter\n",
    "total_size_mb = total_size_bytes / (1024*1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_CONFIG_124M\n",
    "# Number of trainable parameters considering weight tying: 124,412,160\n",
    "# Block: 1, parameters in attn: 2,360,064\n",
    "# Block: 1, parameters in ffn: 4,722,432\n",
    "# Total size of the model: 621.83 MB\n",
    "\n",
    "# GPT_CONFIG_MEDIUM\n",
    "# Number of trainable parameters considering weight tying: 354,749,440\n",
    "# Block: 1, parameters in attn: 4,195,328\n",
    "# Block: 1, parameters in ffn: 8,393,728\n",
    "# Total size of the model: 1549.58 MB\n",
    "\n",
    "# GPT_CONFIG_LARGE\n",
    "# Number of trainable parameters considering weight tying: 773,891,840\n",
    "# Block: 1, parameters in attn: 6,554,880\n",
    "# Block: 1, parameters in ffn: 13,113,600\n",
    "# Total size of the model: 3197.56 MB\n",
    "\n",
    "# GPT_CONFIG_XL\n",
    "# Crashed RAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
